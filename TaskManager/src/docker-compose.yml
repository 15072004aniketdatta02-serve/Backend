version: '3.8'

networks:
  frontend_network:
    driver: bridge
  backend_network:
    driver: bridge
    internal: true
  db_network:
    driver: bridge
    internal: true
  kafka_network:
    driver: bridge
    internal: true

volumes:
  mongodb_primary_data:
    driver: local
  mongodb_secondary1_data:
    driver: local
  mongodb_secondary2_data:
    driver: local
  nginx_logs:
    driver: local
  nginx_cache:
    driver: local
  kafka_data:
    driver: local
  zookeeper_data:
    driver: local
  zookeeper_logs:
    driver: local

services:
  # ==================== NGINX REVERSE PROXY ====================
  nginx:
    image: nginx:alpine
    container_name: nginx_proxy
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
      - nginx_cache:/var/cache/nginx
    networks:
      - frontend_network
      - backend_network
    depends_on:
      - frontend
      - middleware
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M

  # ==================== FRONTEND ====================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        NODE_ENV: production
    container_name: frontend_app
    restart: always
    expose:
      - "3000"
    environment:
      - NODE_ENV=production
      - API_URL=http://middleware:4000
    networks:
      - frontend_network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s

  # ==================== MIDDLEWARE/API ====================
  middleware:
    build:
      context: ./middleware
      dockerfile: Dockerfile
      args:
        NODE_ENV: production
    container_name: middleware_api
    restart: always
    expose:
      - "4000"
    environment:
      - NODE_ENV=production
      - PORT=4000
      - DB_HOST=maxscale
      - DB_PORT=4006
      - DB_NAME=${DB_NAME:-app_database}
      - DB_USER=${DB_USER:-app_user}
      - DB_PASS=${DB_PASS:-secure_password}
      - JWT_SECRET=${JWT_SECRET:-change_this_secret}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis_secure_pass}
      - KAFKA_BROKERS=kafka1:9092,kafka2:9092,kafka3:9092
      - KAFKA_CLIENT_ID=middleware-api
    networks:
      - backend_network
      - db_network
      - kafka_network
    depends_on:
      maxscale:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s

  # ==================== BULLMQ WORKERS ====================
  bullmq-worker:
    build:
      context: ./workers
      dockerfile: Dockerfile
      args:
        NODE_ENV: production
    container_name: bullmq_worker
    restart: always
    environment:
      - NODE_ENV=production
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis_secure_pass}
      - DB_HOST=maxscale
      - DB_PORT=4006
      - DB_NAME=${DB_NAME:-app_database}
      - DB_USER=${DB_USER:-app_user}
      - DB_PASS=${DB_PASS:-secure_password}
      - KAFKA_BROKERS=kafka1:9092,kafka2:9092,kafka3:9092
      - WORKER_CONCURRENCY=5
    networks:
      - backend_network
      - db_network
      - kafka_network
    depends_on:
      redis:
        condition: service_healthy
      kafka1:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "node", "-e", "process.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s

  # ==================== BULLMQ BOARD (UI) ====================
  bullmq-board:
    image: deadly0/bull-board:latest
    container_name: bullmq_board
    restart: always
    ports:
      - "3002:3000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis_secure_pass}
      - BULL_PREFIX=bull
    networks:
      - backend_network
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M

  # ==================== MONGODB REPLICA SET ====================
  mongodb_primary:
    image: mongo:7.0
    container_name: mongodb_primary
    restart: always
    command: mongod --replSet rs0 --bind_ip_all --port 27017
    expose:
      - "27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_ROOT_USER:-admin}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_ROOT_PASS:-secure_mongo_password}
      - MONGO_INITDB_DATABASE=${DB_NAME:-app_database}
    volumes:
      - mongodb_primary_data:/data/db
      - ./mongodb/init-replica.sh:/docker-entrypoint-initdb.d/init-replica.sh:ro
    networks:
      - db_network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

  mongodb_secondary1:
    image: mongo:7.0
    container_name: mongodb_secondary1
    restart: always
    command: mongod --replSet rs0 --bind_ip_all --port 27017
    expose:
      - "27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_ROOT_USER:-admin}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_ROOT_PASS:-secure_mongo_password}
    volumes:
      - mongodb_secondary1_data:/data/db
    networks:
      - db_network
    depends_on:
      - mongodb_primary
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

  mongodb_secondary2:
    image: mongo:7.0
    container_name: mongodb_secondary2
    restart: always
    command: mongod --replSet rs0 --bind_ip_all --port 27017
    expose:
      - "27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_ROOT_USER:-admin}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_ROOT_PASS:-secure_mongo_password}
    volumes:
      - mongodb_secondary2_data:/data/db
    networks:
      - db_network
    depends_on:
      - mongodb_primary
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

  # ==================== MAXSCALE DB PROXY ====================
  maxscale:
    build:
      context: ./maxscale
      dockerfile: Dockerfile
    container_name: maxscale_proxy
    restart: always
    expose:
      - "4006"
      - "4008"
      - "4009"
    volumes:
      - ./maxscale/maxscale.cnf:/etc/maxscale.cnf:ro
    networks:
      - db_network
    depends_on:
      mongodb_primary:
        condition: service_healthy
      mongodb_secondary1:
        condition: service_healthy
      mongodb_secondary2:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "maxctrl", "show", "servers"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # ==================== REDIS CACHE ====================
  redis:
    image: redis:7-alpine
    container_name: redis_cache
    restart: always
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis_secure_pass} --maxmemory 512mb --maxmemory-policy allkeys-lru
    expose:
      - "6379"
    volumes:
      - ./redis/data:/data
    networks:
      - backend_network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # ==================== ZOOKEEPER FOR KAFKA ====================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    restart: always
    expose:
      - "2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 5
      ZOOKEEPER_INIT_LIMIT: 10
      ZOOKEEPER_MAX_CLIENT_CNXNS: 0
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 3
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - kafka_network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # ==================== KAFKA CLUSTER ====================
  kafka1:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka1
    restart: always
    expose:
      - "9092"
      - "9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:9092,EXTERNAL://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_COMPRESSION_TYPE: snappy
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: kafka1
    volumes:
      - ./kafka/kafka1:/var/lib/kafka/data
    networks:
      - kafka_network
      - backend_network
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  kafka2:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka2
    restart: always
    expose:
      - "9092"
      - "9094"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:9092,EXTERNAL://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_COMPRESSION_TYPE: snappy
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: kafka2
    volumes:
      - ./kafka/kafka2:/var/lib/kafka/data
    networks:
      - kafka_network
      - backend_network
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  kafka3:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka3
    restart: always
    expose:
      - "9092"
      - "9095"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9095
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka3:9092,EXTERNAL://localhost:9095
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_COMPRESSION_TYPE: snappy
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: kafka3
    volumes:
      - ./kafka/kafka3:/var/lib/kafka/data
    networks:
      - kafka_network
      - backend_network
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # ==================== KAFKA UI ====================
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka_ui
    restart: always
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: production
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:9092,kafka2:9092,kafka3:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_METRICS_PORT: 9999
      DYNAMIC_CONFIG_ENABLED: "true"
    networks:
      - kafka_network
    depends_on:
      kafka1:
        condition: service_healthy
      kafka2:
        condition: service_healthy
      kafka3:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # ==================== KAFKA CONSUMER GROUPS ====================
  kafka-consumer:
    build:
      context: ./kafka-consumers
      dockerfile: Dockerfile
    container_name: kafka_consumer
    restart: always
    environment:
      - NODE_ENV=production
      - KAFKA_BROKERS=kafka1:9092,kafka2:9092,kafka3:9092
      - KAFKA_GROUP_ID=app-consumer-group
      - KAFKA_CLIENT_ID=consumer-service
      - DB_HOST=maxscale
      - DB_PORT=4006
      - DB_NAME=${DB_NAME:-app_database}
      - DB_USER=${DB_USER:-app_user}
      - DB_PASS=${DB_PASS:-secure_password}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis_secure_pass}
    networks:
      - kafka_network
      - db_network
      - backend_network
    depends_on:
      kafka1:
        condition: service_healthy
      maxscale:
        condition: service_healthy
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s

  # ==================== MONITORING ====================
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: always
    expose:
      - "9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - backend_network
      - kafka_network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: always
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASS:-admin_password}
      - GF_INSTALL_PLUGINS=grafana-clock-panel
    volumes:
      - ./grafana/data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - backend_network
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
          #docker-compose up --scale web=3 -d
          